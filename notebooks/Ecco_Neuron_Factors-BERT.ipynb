{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jalammar/ecco/blob/main/notebooks/Ecco_Neuron_Factors-BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CLDsAru2N0lr"
      },
      "outputs": [],
      "source": [
        "#!pip install ecco"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sknc5dj3N1LS"
      },
      "outputs": [],
      "source": [
        "import ecco\n",
        "# Other options are: 'distilbert-base-uncased' and 'bert-large-uncased'\n",
        "#lm = ecco.from_pretrained('distilgpt2', activations=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Other options are: 'distilbert-base-uncased' and 'bert-large-uncased'\n",
        "lm = ecco.from_pretrained('distilbert-base-uncased', activations=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18\"\n",
        "inputs = lm.tokenizer([text], return_tensors=\"pt\")\n",
        "output = lm(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "poem = \"\"\"\n",
        "The road not taken\n",
        "\n",
        "Two roads diverged in a yellow wood,\n",
        "And sorry I could not travel both\n",
        "And be one traveler, long I stood\n",
        "And looked down one as far as I could\n",
        "To where it bent in the undergrowth;\n",
        "\n",
        "Then took the other, as just as fair,\n",
        "And having perhaps the better claim,\n",
        "Because it was grassy and wanted wear;\n",
        "Though as for that the passing there\n",
        "Had worn them really about the same,\n",
        "\n",
        "And both that morning equally lay\n",
        "In leaves no step had trodden black.\n",
        "Oh, I kept the first for another day!\n",
        "Yet knowing how way leads on to way,\n",
        "I doubted if I should ever come back.\n",
        "\n",
        "I shall be telling this with a sigh\n",
        "Somewhere ages and ages hence:\n",
        "Two roads diverged in a wood, and Iâ€”\n",
        "I took the one less traveled by,\n",
        "And that has made all the difference.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "some_code = \"\"\"\n",
        "for i in range(10):\n",
        "\tprint(\"Hello, world!\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "some_code = \"\"\"\n",
        "import openai\n",
        "import numpy as np\n",
        "from ecco.prompts import prompt_fs\n",
        "\n",
        "class Explainer:\n",
        "\tdef __init__(self, nmf_object, threshold = 0.1, api_key = \"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\"):\n",
        "\t\topenai.api_key = api_key\n",
        "\t\tself.components = nmf_object.components \n",
        "\t\tself.tokens = nmf_object.tokens[0]\n",
        "\t\tself.num_of_tokens = len(nmf_object.tokens[0])\n",
        "\t\tself.threshold = threshold\n",
        "\n",
        "\tdef create_masked_token_list(self, masked_activations):\n",
        "\t\tlist_of_masked_tokens = [self.tokens[x] if masked_activations[x] != 0 else \"_\" for x in range(self.num_of_tokens)]\n",
        "\t\treturn list_of_masked_tokens\n",
        "\n",
        "\tdef create_string_result(self, masked_strings):\n",
        "\t\tresult_string = \"The original string input was: '\"\n",
        "\t\toriginal_input = \" \".join(self.tokens)\n",
        "\t\tn_of_factors = len(masked_strings)\n",
        "\t\tresult_string = result_string + original_input + \"'. \\n\"\n",
        "\t\t#result_string = result_string + f\"The activations used to mask tokens was found with a threshold of {threshold}. \\n\"\n",
        "\t\tresult_string = result_string + f\"The activations were decomposed into {n_of_factors} factors. \\n\"\n",
        "\t\t#result_string = result_string + \"Any mask is marked by replacing the original text with an underscore: '_' \"\n",
        "\t\tfor i in range(len(masked_strings)):\n",
        "\t\t\tstring_to_add = f\"Factor number {i+1} contained these tokens: '\"\n",
        "\t\t\t#masked_string = \"\".join(masked_strings[i][1])\n",
        "\t\t\t# changed since create_Maskedd_token_lists no longer returns \"words\" AND \"masked lists\"\n",
        "\t\t\tmasked_string = \" \".join(masked_strings[i])\n",
        "\t\t\tstring_to_add = string_to_add + masked_string + \"'. \\n\"\n",
        "\t\t\tresult_string = result_string + string_to_add\n",
        "\t\treturn result_string\n",
        "\n",
        "\"\"\"\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = some_code \n",
        "inputs = lm.tokenizer([text], return_tensors=\"pt\")\n",
        "output = lm(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "l171rfDeBff-"
      },
      "outputs": [],
      "source": [
        "# Factorize activations in all the layers\n",
        "nmf_1 = output.run_nmf(n_components=10) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You will act as an explainability module, providing a narrative explanation the relationship between several strings containing words.\n",
            "You will receive several strings of initial text, but with masks.\n",
            "A single masked token is marked by a single underscore.\n",
            "Several underscores mean that several tokens have been replaced.\n",
            "The tokens, which have not been replaced by an underscore, has been shown to be activated by some cluster of neurons of the large language model, in response to the input shown earlier.\n",
            "Your job is to describe any relationship which might be present between the remaining tokens, which are not replaced by underscores.\n",
            "These relationships might be syntactic, grammatic, semantic or some special token of the large language model like \"[CLS]\" and \"[SEP]\".\n",
            "Just saying that \"these factors relate to words\" is insufficient. You must figure out what these tokens have in common.\n",
            "Or they might just be selectively attending to particular parts of a sentence like the start, middle or the end.\n",
            "There might be some tokens which don't seem to be connected to the rest of the tokens, even if they are not blanked out.\n",
            "You are allowed to summarize the general tendencies of the tokens which have not been blanked out, while skipping a smaller portion of these tokens, if they don't seem to be related to the rest of the tokens.\n",
            "\n",
            "If there is no clear relationship between any of the tokens of a factor you should write: \"I could not find an intuitive connection between the tokens of this factor\". \n",
            "\n",
            "Below I will provide some examples:\n",
            "Example 1:\n",
            "\n",
            "Follow the advice and examples as closely as possible for the following input text:The original string input was: '[CLS] < page > < title > anti ##christ < / title > < id > 86 ##5 < / id > < revision > < id > 159 ##00 ##6 ##7 ##6 < / id > < times ##tam ##p > 2002 - 08 - 03 ##t ##18 : 14 : 12 ##z < / times ##tam ##p > < contributor > < user ##name > paris < / user ##name > < id > 23 < / id > < / contributor > < minor / > < comment > automated ##con ##version < / comment > < text ##x ##ml : space = \" preserve \" > # red ##ire ##ct [ [ christianity ] ] < / text > < / revision > < / page > [SEP]'. \n",
            "The activations were decomposed into 10 factors. \n",
            "Factor number 1 contained these tokens: '_ _ _ _ _ _ _ anti _ _ _ _ _ _ _ _ 86 _ _ _ _ _ _ _ _ _ _ > _ ##00 ##6 ##7 _ _ _ _ _ _ times ##tam ##p _ _ _ _ _ 03 ##t _ _ _ _ 12 ##z _ _ times ##tam ##p _ _ _ _ _ user ##name _ _ _ _ user ##name _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ automated ##con _ _ _ _ _ _ text ##x _ _ _ _ \" _ _ _ _ red ##ire ##ct _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _'. \n",
            "Factor number 2 contained these tokens: '[CLS] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ [SEP]'. \n",
            "Factor number 3 contained these tokens: '_ _ _ _ < _ _ _ _ _ / title _ < _ _ _ ##5 _ / _ _ < _ _ < _ _ _ _ _ _ ##6 _ / _ _ _ _ _ ##p _ 2002 _ _ _ _ _ _ _ _ _ _ _ _ / _ _ ##p _ < _ _ < user ##name _ _ _ / user ##name _ < _ _ _ _ / _ _ _ / _ _ < _ / _ < _ _ _ _ _ _ / _ _ _ _ _ ##ml : _ = \" _ _ _ # _ _ _ _ [ _ _ _ _ / _ _ _ / _ _ _ / page _ _'. \n",
            "Factor number 4 contained these tokens: '[CLS] _ page _ < title _ _ _ _ _ title _ _ _ > _ _ < / _ _ < _ _ _ _ > _ ##00 ##6 ##7 ##6 < / _ > < times _ ##p > _ - 08 - _ ##t _ : 14 : _ ##z < / _ _ ##p > < _ _ _ _ ##name > paris _ _ _ ##name _ < _ > _ _ / _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ text _ _ : _ = \" _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ page _ _'. \n",
            "Factor number 5 contained these tokens: '[CLS] _ _ _ _ _ _ anti _ _ _ _ _ _ _ _ _ ##5 _ _ _ _ _ _ _ _ _ _ _ _ _ _ ##6 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ : 14 _ _ ##z _ _ _ _ ##p _ _ _ _ _ _ _ _ paris _ _ _ _ > < id > 23 < / id > < / _ > < minor / > < comment > automated ##con ##version < / comment > < text ##x ##ml : space = \" preserve \" > # red ##ire ##ct [ [ christianity ] ] < / text > < / revision > < / page > _'. \n",
            "Factor number 6 contained these tokens: '_ _ page > _ _ > _ _ _ _ _ > _ _ > _ ##5 _ _ _ > _ _ > _ _ > _ _ _ _ ##6 _ _ _ > _ _ _ ##p > _ _ _ _ _ _ ##18 _ _ _ _ _ _ _ _ _ ##p > _ _ > _ _ ##name > _ _ _ _ ##name > _ _ > _ _ _ _ > _ _ _ > _ _ / > _ _ > _ _ _ _ _ _ > _ _ _ _ _ _ = _ _ \" > _ _ _ _ _ _ _ ] ] _ _ _ > _ _ _ > _ _ _ > _'. \n",
            "Factor number 7 contained these tokens: '[CLS] < page > < title > anti ##christ < / title > < _ > 86 _ _ _ _ _ _ revision _ _ _ _ 159 _ _ _ _ _ _ _ _ _ times ##tam ##p _ 2002 _ 08 _ _ _ ##18 : 14 : _ _ _ _ times _ ##p _ _ contributor _ _ user ##name _ paris _ _ user ##name _ _ _ _ 23 < _ _ _ < _ contributor _ _ minor / > _ comment _ automated ##con ##version < _ comment _ < text _ ##ml : space = _ preserve _ _ # red _ ##ct _ _ christianity _ _ _ _ text > < _ revision > < _ page _ _'. \n",
            "Factor number 8 contained these tokens: '[CLS] _ _ _ _ _ _ anti ##christ _ _ _ _ _ id > 86 ##5 < / id _ _ revision _ _ _ _ 159 ##00 ##6 ##7 ##6 < _ _ _ _ _ _ _ _ 2002 - 08 - 03 ##t ##18 _ 14 _ 12 ##z _ _ _ _ _ _ _ contributor _ _ _ _ _ paris _ _ _ _ _ _ _ _ 23 < _ _ _ _ _ contributor _ _ minor _ _ _ _ _ automated ##con ##version _ _ _ _ _ _ ##x ##ml _ _ _ _ preserve _ _ # red ##ire ##ct _ _ christianity _ _ _ _ _ _ _ _ revision _ _ _ _ _ _'. \n",
            "Factor number 9 contained these tokens: '_ < _ _ < _ _ anti _ < _ _ _ < _ _ _ _ < _ _ _ < _ _ < _ _ _ ##00 _ _ _ < _ _ _ < _ _ _ _ _ _ 08 _ _ _ _ _ _ : _ _ < _ _ _ _ _ < _ _ < user _ _ _ < _ _ _ _ < _ _ _ < _ _ _ < _ _ _ < _ _ > < _ _ _ _ _ < _ _ _ < _ _ _ _ _ = _ _ _ _ # _ _ _ [ [ _ _ _ < _ _ _ < _ _ _ < _ _ _ _'. \n",
            "Factor number 10 contained these tokens: '[CLS] _ page _ _ title _ anti ##christ _ _ title _ _ id _ 86 _ _ _ id _ < revision _ < id _ _ _ _ _ _ _ / id _ _ _ _ ##p _ _ _ _ _ _ ##t ##18 : 14 _ _ ##z _ _ times ##tam ##p _ < _ _ _ _ ##name _ _ _ _ _ ##name _ < id > _ _ _ id _ _ _ contributor _ _ minor / > < _ _ _ _ _ _ _ comment _ _ _ ##x ##ml _ space _ _ _ \" _ # _ _ ##ct _ _ _ _ _ _ _ text _ _ _ revision _ _ _ page _ _'. \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'test response'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nmf_1.explain()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nmf_2 = output.run_nmf(n_components=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nmf_2.explore()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "The original string input was: '[CLS]theroadnottakentworoadsdiver##gedinayellowwood,andsorryicouldnottravelbothandbeonetraveler,longistoodandlookeddownoneasfarasicouldtowhereitbentintheunder##growth;thentooktheother,asjustasfair,andhavingperhapsthebetterclaim,becauseitwasgrassyandwantedwear;thoughasforthatthepassingtherehadwornthemreallyaboutthesame,andboththatmorningequallylayinleavesnostephadtr##od##denblack.oh,ikeptthefirstforanotherday!yetknowinghowwayleadsontoway,idoubtedifishouldevercomeback.ishallbetellingthiswithasighsomewhereagesandageshence:tworoadsdiver##gedinawood,andiâ€”itooktheonelesstraveledby,andthathasmadeallthedifference.[SEP]'. \n",
        "The activations were decomposed into 10 factors. \n",
        "Factor number 1 contained these tokens: '_______________sorry______be__,long_stood_lookeddownoneasfarasicouldtowhereitbentin_under_;thentook_other,asjustasfair,andhavingperhapsthebetterclaim,becauseitwasgrassyandwantedwear;thoughasforthat______reallyaboutthesame_____equally_________________for____knowinghowway_on_____if___________with__somewhereages_ageshence:_______________less_by____madeall_difference__'. \n",
        "Factor number 2 contained these tokens: '[CLS]the_not_________,andsorry__not__and___,longistoodand_________where______;then____asjust___andhavingperhaps____because_was_andwanted_;thoughasforthatthe__had_______andboththat_____no_had_____oh,_______!yet__________if______ishallbetellingthiswithasighsomewhereagesandageshence:____in__,andiâ€”itookthe_less_by,andthathas_all__._'. \n",
        "Factor number 3 contained these tokens: '________________________________________________________,______,______________________,_______________._________!________,________.________________________________,_______._'. \n",
        "Factor number 4 contained these tokens: '[CLS]theroadnottaken_roadsdiver##ged__yellowwood__sorryicould_travelboth_be_traveler_longistood_lookeddown__far_icould_whereitbent_theunder##growth;thentook_other__just_fair__havingperhaps_betterclaim__it_grassy_wantedwear;_____passing__wornthemreallyabout_same____morningequallylay_leaves_step_tr##od##denblack____kept____day__knowing_wayleads__way__doubted___evercomeback__shall_tellingthis__sighsomewhereagesandageshence:tworoadsdiver##ged__wood__iâ€”itooktheonelesstraveledby,___made__difference._'. \n",
        "Factor number 5 contained these tokens: '[CLS]theroadnot___________sorryicouldnot___be__,longistoodandlooked____asicouldto_it_____;thentook_other__just_____perhaps____becauseit_____;______therehad_them_____________no_______oh,i______!_________i_ifishould_comeback_ishallbe_thiswithasigh_____:tworoads_##ged__wood,andiâ€”itook_one_traveled___thathas_allthe_._'. \n",
        "Factor number 6 contained these tokens: '_theroadnottakentwo_diver##gedinayellowwood,and_icouldnot_bothandbeone_,longistoodandlookeddownone_farasicouldtowhereitbentintheunder_;thentooktheother,_just____having______itwas_and_____for___there__them_about____both____in_no____##den_______firstforanother____how__ontoway___if__evercomeback.__betellingthiswitha_somewhere_and_hence:two_diver##gedina_,and_â€”_tooktheoneless_by__that__all__._'. \n",
        "Factor number 7 contained these tokens: '___nottaken__________sorry_couldnottravelboth_be___long_stood_lookeddown__far__couldtowhere_bent____;thentook_other__just_fair__havingperhaps_betterclaim_becauseitwasgrassyandwantedwear;thoughasforthatthepassingtherehadwornthemreallyaboutthesame,andboththatmorningequallylayinleavesnostephadtr##od##denblack.oh,ikeptthefirstforanotherday!yetknowinghowwayleadsontoway,idoubtedifishouldevercomeback_ishallbetellingthiswithasighsomewhereagesandageshence:___##ged______â€”_took_onelesstraveledby__thathasmadeallthedifference__'. \n",
        "Factor number 8 contained these tokens: '[CLS]________________________,____________________;____________perhaps___________;_______________________________oh,__________________________ishall__this_a______:_______________________allthe_.[SEP]'. \n",
        "Factor number 9 contained these tokens: '_the________a__________andbeone_,______________it__theunder##growth;thentooktheother,_just_fair___perhapsthebetterclaim__itwasgrassyandwantedwear;__forthatthepassingtherehad_themreallyaboutthesame__boththat_____no___________the_foranother____how___to_______________this_a__agesandages_______a_______theoneless_by__thathasmadeallthedifference._'. \n",
        "Factor number 10 contained these tokens: '[CLS]___________wood,______both____,_istood_________to___in___;___other,_just_fair,_havingperhaps__claim,_it____wear;though_forthat__there__themreallyabout_same,__that_equally_____had___black_oh,_______!________,_______back_____thiswith______hence:____in_wood,_iâ€”____less_by,_that_____._'. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nmf_2.components[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mask_elements_below_threshold(components_list, threshold):\n",
        "\tmasked_array = np.array([[0 if y < threshold else y for y in components_list[x]] for x in range(len(components_list))])\n",
        "\treturn masked_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def for_loop_create_masked_token_list(words, masked_activations):\n",
        "\tnew_list = []\n",
        "\tfor i in range(len(words)):\n",
        "\t\tif masked_activations[i] != 0:\n",
        "\t\t\tnew_list.append(words[i])\n",
        "\t\telse:\n",
        "\t\t\tnew_list.append(\"_\")\n",
        "\treturn new_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_masked_token_list(words, masked_activations):\n",
        "    list_of_masked_tokens = [words[x] if masked_activations[x] != 0 else \"_\" for x in range(len(words))]\n",
        "    return words, list_of_masked_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_masked_token_lists(words, masked_activations):\n",
        "\tmasked_lists = [create_masked_token_list(words, masked_activations[x]) for x in range(len(masked_activations))]\n",
        "\treturn masked_lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_string_result(words, masked_strings, threshold, nmf):\n",
        "\tresult_string = \"The original string input was: '\"\n",
        "\toriginal_input = \"\".join(nmf.tokens[0])\n",
        "\tn_of_factors = len(masked_strings)\n",
        "\tresult_string = result_string + original_input + \"'. \\n\"\n",
        "\tresult_string = result_string + f\"The activations used to mask tokens was found with a threshold of {threshold}. \\n\"\n",
        "\tresult_string = result_string + f\"The activations were decomposed into {n_of_factors} factors. \\n\"\n",
        "\tresult_string = result_string + \"Any mask is marked by replacing the original text with an underscore: '_' \"\n",
        "\tfor i in range(len(masked_strings)):\n",
        "\t\tstring_to_add = f\"Factor number {i+1} contained these tokens: '\"\n",
        "\t\tmasked_string = \"\".join(masked_strings[i][1])\n",
        "\t\tstring_to_add = string_to_add + masked_string + \"'. \\n\"\n",
        "\t\tresult_string = result_string + string_to_add\n",
        "\n",
        "\treturn result_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "threshold = 0.01\n",
        "masked_list_of_activations = mask_elements_below_threshold(nmf_1.components, threshold)\n",
        "masked_token_lists = create_masked_token_lists(nmf_1.tokens[0], masked_list_of_activations)\n",
        "result_string = create_string_result(nmf_1.tokens[0], masked_token_lists, threshold, nmf_1)\n",
        "result_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "threshold = 0.01\n",
        "masked_list_of_activations = mask_elements_below_threshold(nmf_2.components, threshold)\n",
        "masked_token_lists = create_masked_token_lists(nmf_2.tokens[0], masked_list_of_activations)\n",
        "result_string = create_string_result(nmf_2.tokens[0], masked_token_lists, threshold, nmf_2)\n",
        "result_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# example with countries\n",
        "input_text = \"\"\"The countries of the European Union are:\n",
        "1.Austria\n",
        "2.Belgium\n",
        "3.Bulgaria\n",
        "4.Croatia\n",
        "5.Cyprus\n",
        "6.CzechRepublic\n",
        "7.Denmark\"\n",
        "\"\"\"\n",
        "threshold = 0.01\n",
        "input = lm.tokenizer([input_text], return_tensors=\"pt\")\n",
        "output = lm(input)\n",
        "nmf_3 = output.run_nmf(n_components=6)\n",
        "masked_list_of_activations = mask_elements_below_threshold(nmf_3.components, threshold)\n",
        "masked_token_lists = create_masked_token_lists(nmf_3.tokens[0], masked_list_of_activations)\n",
        "result_string = create_string_result(nmf_3.tokens[0], masked_token_lists, threshold, nmf_3)\n",
        "result_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# example with XML\n",
        "input_text = \"\"\"\n",
        "<page>\\n <title>Antichrist</title>\\n <id>865</id>\\n <revision>\\n   <id>15900676</id>\\n   <timestamp>2002-08-03T18:14:12Z</timestamp>\\n   <contributor>\\n     <username>Paris</username>\\n     <id>23</id>\\n   </contributor>\\n   <minor/>\\n   <comment>Automatedconversion</comment>\\n   <textxml:space=\"preserve\">#REDIRECT[[Christianity]]</text>\\n </revision>\\n</page>\n",
        "\"\"\"\n",
        "threshold = 0.01\n",
        "input = lm.tokenizer([input_text], return_tensors=\"pt\")\n",
        "output = lm(input)\n",
        "nmf_4 = output.run_nmf(n_components=10)\n",
        "masked_list_of_activations = mask_elements_below_threshold(nmf_4.components, threshold)\n",
        "masked_token_lists = create_masked_token_lists(nmf_4.tokens[0], masked_list_of_activations)\n",
        "result_string = create_string_result(nmf_4.tokens[0], masked_token_lists, threshold, nmf_4)\n",
        "result_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nmf_4.explore()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#use gpt model to explain factors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# this requires you to already ahve instantiated a language model from the ecco library with acvtivations = True\n",
        "def nmf_auto_analysis(input_string, n_of_factors, language_model):\n",
        "\t# maybe check if language_model exists??\n",
        "\t# check if input string is string?\n",
        "\tlm = language_model\n",
        "\tinputs = lm.tokenizer([input_string], return_tensors=\"pt\")\n",
        "\toutput = lm(inputs)\n",
        "\n",
        "\treturn string_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# iterate through each component, and then through each token, returning a list of the activations for each component for each word\n",
        "complicated_example = [[nmf_1.components[num1][num2] for num1 in range(len(nmf_1.components))] for num2 in range (len(nmf_1.components[0]))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_prompt_from_list_of_words(string_of_words):\n",
        "\tstart = \"I will provide a list of comma-separated words/tokens, each contained within a set of quotation marks. Please return the 3 grammatical, syntactic or semantic relationships between these tokens which best describes any relationship between the tokens. Be very brief. I will provide some context, to improve the quality of your response, but you should not mention this in your response - exclusively give me back the 3 most central commonalities/relationships between the words/tokens, and afterwards say how certain you are. Each of these words selectively responded to a cluster of artificial neurons of a large language model. Here is the list of words: \"\n",
        "\tprompt = start + string_of_words\n",
        "\treturn prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#response = (openai.Completion.create(model=\"text-davinci-003\", prompt=test_prompt, temperature=0.2, max_tokens=400))[\"choices\"][0][\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from prompts import prompt_fs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_prompt = prompt_fs\n",
        "test_prompt = test_prompt.prompt.strip()\n",
        "test_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_text = \"\"\"GPT is a large language model, and it's use is becoming increasingly widespread. \n",
        "As the use of GPT-based models increases, so does their use of tools.\n",
        "An interesting avenue of research is analysis of how these models are able to use tools.\"\"\"\n",
        "\n",
        "threshold = 0.01\n",
        "input = lm.tokenizer([input_text], return_tensors=\"pt\")\n",
        "output = lm(input)\n",
        "nmf_5 = output.run_nmf(n_components=6)\n",
        "masked_list_of_activations = mask_elements_below_threshold(nmf_5.components, threshold)\n",
        "masked_token_lists = create_masked_token_lists(nmf_5.tokens[0], masked_list_of_activations)\n",
        "result_string = create_string_result(nmf_5.tokens[0], masked_token_lists, threshold, nmf_5)\n",
        "prompt_for_gpt = test_prompt + result_string\n",
        "prompt_for_gpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_prompt(results_string):\n",
        "\ttest_prompt = prompt_fs\n",
        "\ttest_prompt = test_prompt.prompt.strip()\n",
        "\tprompt = test_prompt + results_string\n",
        "\treturn prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def conduct_nmf(input, n_components):\n",
        "\tinput = lm.tokenizer([input], return_tensors=\"pt\")\n",
        "\toutput = lm(input)\n",
        "\tnmf = output.run_nmf(n_components=n_components)\n",
        "\treturn nmf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze(input, n_components, threshold):\n",
        "\tnmf = conduct_nmf(input, n_components)\n",
        "\tresult_string = create_string_result(nmf.tokens[0], create_masked_token_lists(nmf.tokens[0],mask_elements_below_threshold(nmf.components, threshold)), threshold, nmf)\n",
        "\tprompt = create_prompt(result_string)\n",
        "\tresponse = (openai.Completion.create(model=\"text-davinci-003\", prompt=prompt, temperature=0.5, max_tokens=600))[\"choices\"][0][\"text\"]\n",
        "\treturn response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "openai.api_key = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input = \"\"\n",
        "response = analyze(input, 3, 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nmf_5.explore()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = (openai.Completion.create(model=\"text-davinci-003\", prompt=prompt_for_gpt, temperature=0.5, max_tokens=600))[\"choices\"][0][\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyM3g0GHWeQGHxRkn4PodvXC",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "05- Neuron Factors.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
