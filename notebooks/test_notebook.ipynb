{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ecco\n",
    "# Other options are: 'distilbert-base-uncased' and 'bert-large-uncased'\n",
    "#lm = ecco.from_pretrained('distilgpt2', activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Other options are: 'distilbert-base-uncased' and 'bert-large-uncased'\n",
    "#lm = ecco.from_pretrained('distilbert-base-uncased', activations=True)\n",
    "lm = ecco.from_pretrained('distilbert-base-uncased', activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18\"\n",
    "inputs = lm.tokenizer([text], return_tensors=\"pt\")\n",
    "output = lm(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem = \"\"\"\n",
    "The road not taken\n",
    "\n",
    "Two roads diverged in a yellow wood,\n",
    "And sorry I could not travel both\n",
    "And be one traveler, long I stood\n",
    "And looked down one as far as I could\n",
    "To where it bent in the undergrowth;\n",
    "\n",
    "Then took the other, as just as fair,\n",
    "And having perhaps the better claim,\n",
    "Because it was grassy and wanted wear;\n",
    "Though as for that the passing there\n",
    "Had worn them really about the same,\n",
    "\n",
    "And both that morning equally lay\n",
    "In leaves no step had trodden black.\n",
    "Oh, I kept the first for another day!\n",
    "Yet knowing how way leads on to way,\n",
    "I doubted if I should ever come back.\n",
    "\n",
    "I shall be telling this with a sigh\n",
    "Somewhere ages and ages hence:\n",
    "Two roads diverged in a wood, and I—\n",
    "I took the one less traveled by,\n",
    "And that has made all the difference.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem = \"\"\"\n",
    " it sits outside my window now\n",
    "like and old woman going to market;\n",
    "it sits and watches me,\n",
    "it sweats nevously\n",
    "through wire and fog and dog-bark\n",
    "until suddenly\n",
    "I slam the screen with a newspaper\n",
    "like slapping at a fly\n",
    "and you could hear the scream\n",
    "over this plain city,\n",
    "and then it left.\n",
    "\n",
    "\n",
    "the way to end a poem\n",
    "like this\n",
    "is to become suddenly\n",
    "quiet.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = poem\n",
    "inputs = lm.tokenizer([text], return_tensors=\"pt\")\n",
    "output = lm(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorize activations in all the layers\n",
    "nmf_1 = output.run_nmf(n_components=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html lang=\"en\">\n",
       "<script src=\"https://requirejs.org/docs/release/2.3.6/minified/require.js\"></script>\n",
       "<script>\n",
       "    var ecco_url = 'https://storage.googleapis.com/ml-intro/ecco/'\n",
       "    //var ecco_url = 'http://localhost:8000/'\n",
       "\n",
       "    if (window.ecco === undefined) window.ecco = {}\n",
       "\n",
       "    // Setup the paths of the script we'll be using\n",
       "    requirejs.config({\n",
       "        urlArgs: \"bust=\" + (new Date()).getTime(),\n",
       "        nodeRequire: require,\n",
       "        paths: {\n",
       "            d3: \"https://d3js.org/d3.v6.min\", // This is only for use in setup.html and basic.html\n",
       "            \"d3-array\": \"https://d3js.org/d3-array.v2.min\",\n",
       "            jquery: \"https://code.jquery.com/jquery-3.5.1.min\",\n",
       "            ecco: ecco_url + 'js/0.0.6/ecco-bundle.min',\n",
       "            xregexp: 'https://cdnjs.cloudflare.com/ajax/libs/xregexp/3.2.0/xregexp-all.min'\n",
       "        }\n",
       "    });\n",
       "\n",
       "    // Add the css file\n",
       "    //requirejs(['d3'],\n",
       "    //    function (d3) {\n",
       "    //        d3.select('#css').attr('href', ecco_url + 'html/styles.css')\n",
       "    //    })\n",
       "\n",
       "    console.log('Ecco initialize!!')\n",
       "\n",
       "    // returns a 'basic' object. basic.init() selects the html div we'll be\n",
       "    // rendering the html into, adds styles.css to the document.\n",
       "    define('basic', ['d3'],\n",
       "        function (d3) {\n",
       "            return {\n",
       "                init: function (viz_id = null) {\n",
       "                    if (viz_id == null) {\n",
       "                        viz_id = \"viz_\" + Math.round(Math.random() * 10000000)\n",
       "                    }\n",
       "                    // Select the div rendered below, change its id\n",
       "                    const div = d3.select('#basic').attr('id', viz_id),\n",
       "                        div_parent = d3.select('#' + viz_id).node().parentNode\n",
       "\n",
       "                    // Link to CSS file\n",
       "                    d3.select(div_parent).insert('link')\n",
       "                        .attr('rel', 'stylesheet')\n",
       "                        .attr('type', 'text/css')\n",
       "                        .attr('href', ecco_url + 'html/0.0.2/styles.css')\n",
       "\n",
       "                    return viz_id\n",
       "                }\n",
       "            }\n",
       "        }, function (err) {\n",
       "            console.log(err);\n",
       "        }\n",
       "    )\n",
       "</script>\n",
       "\n",
       "<head>\n",
       "    <link id='css' rel=\"stylesheet\" type=\"text/css\">\n",
       "</head>\n",
       "<div id=\"basic\"></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n         requirejs(['basic', 'ecco'], function(basic, ecco){\n            const viz_id = basic.init()\n            \n            ecco.interactiveTokensAndFactorSparklines(viz_id, {'tokens': [{'token': '[CLS]', 'token_id': 101, 'type': 'input', 'position': 0}, {'token': 'it', 'token_id': 2009, 'type': 'input', 'position': 1}, {'token': 'sits', 'token_id': 7719, 'type': 'input', 'position': 2}, {'token': 'outside', 'token_id': 2648, 'type': 'input', 'position': 3}, {'token': 'my', 'token_id': 2026, 'type': 'input', 'position': 4}, {'token': 'window', 'token_id': 3332, 'type': 'input', 'position': 5}, {'token': 'now', 'token_id': 2085, 'type': 'input', 'position': 6}, {'token': 'like', 'token_id': 2066, 'type': 'input', 'position': 7}, {'token': 'and', 'token_id': 1998, 'type': 'input', 'position': 8}, {'token': 'old', 'token_id': 2214, 'type': 'input', 'position': 9}, {'token': 'woman', 'token_id': 2450, 'type': 'input', 'position': 10}, {'token': 'going', 'token_id': 2183, 'type': 'input', 'position': 11}, {'token': 'to', 'token_id': 2000, 'type': 'input', 'position': 12}, {'token': 'market', 'token_id': 3006, 'type': 'input', 'position': 13}, {'token': ';', 'token_id': 1025, 'type': 'input', 'position': 14}, {'token': 'it', 'token_id': 2009, 'type': 'input', 'position': 15}, {'token': 'sits', 'token_id': 7719, 'type': 'input', 'position': 16}, {'token': 'and', 'token_id': 1998, 'type': 'input', 'position': 17}, {'token': 'watches', 'token_id': 12197, 'type': 'input', 'position': 18}, {'token': 'me', 'token_id': 2033, 'type': 'input', 'position': 19}, {'token': ',', 'token_id': 1010, 'type': 'input', 'position': 20}, {'token': 'it', 'token_id': 2009, 'type': 'input', 'position': 21}, {'token': 'sweat', 'token_id': 7518, 'type': 'input', 'position': 22}, {'token': '##s', 'token_id': 2015, 'type': 'input', 'position': 23}, {'token': 'ne', 'token_id': 11265, 'type': 'input', 'position': 24}, {'token': '##vo', 'token_id': 6767, 'type': 'input', 'position': 25}, {'token': '##usly', 'token_id': 27191, 'type': 'input', 'position': 26}, {'token': 'through', 'token_id': 2083, 'type': 'input', 'position': 27}, {'token': 'wire', 'token_id': 7318, 'type': 'input', 'position': 28}, {'token': 'and', 'token_id': 1998, 'type': 'input', 'position': 29}, {'token': 'fog', 'token_id': 9666, 'type': 'input', 'position': 30}, {'token': 'and', 'token_id': 1998, 'type': 'input', 'position': 31}, {'token': 'dog', 'token_id': 3899, 'type': 'input', 'position': 32}, {'token': '-', 'token_id': 1011, 'type': 'input', 'position': 33}, {'token': 'bark', 'token_id': 11286, 'type': 'input', 'position': 34}, {'token': 'until', 'token_id': 2127, 'type': 'input', 'position': 35}, {'token': 'suddenly', 'token_id': 3402, 'type': 'input', 'position': 36}, {'token': 'i', 'token_id': 1045, 'type': 'input', 'position': 37}, {'token': 'slam', 'token_id': 9555, 'type': 'input', 'position': 38}, {'token': 'the', 'token_id': 1996, 'type': 'input', 'position': 39}, {'token': 'screen', 'token_id': 3898, 'type': 'input', 'position': 40}, {'token': 'with', 'token_id': 2007, 'type': 'input', 'position': 41}, {'token': 'a', 'token_id': 1037, 'type': 'input', 'position': 42}, {'token': 'newspaper', 'token_id': 3780, 'type': 'input', 'position': 43}, {'token': 'like', 'token_id': 2066, 'type': 'input', 'position': 44}, {'token': 'slapping', 'token_id': 22021, 'type': 'input', 'position': 45}, {'token': 'at', 'token_id': 2012, 'type': 'input', 'position': 46}, {'token': 'a', 'token_id': 1037, 'type': 'input', 'position': 47}, {'token': 'fly', 'token_id': 4875, 'type': 'input', 'position': 48}, {'token': 'and', 'token_id': 1998, 'type': 'input', 'position': 49}, {'token': 'you', 'token_id': 2017, 'type': 'input', 'position': 50}, {'token': 'could', 'token_id': 2071, 'type': 'input', 'position': 51}, {'token': 'hear', 'token_id': 2963, 'type': 'input', 'position': 52}, {'token': 'the', 'token_id': 1996, 'type': 'input', 'position': 53}, {'token': 'scream', 'token_id': 6978, 'type': 'input', 'position': 54}, {'token': 'over', 'token_id': 2058, 'type': 'input', 'position': 55}, {'token': 'this', 'token_id': 2023, 'type': 'input', 'position': 56}, {'token': 'plain', 'token_id': 5810, 'type': 'input', 'position': 57}, {'token': 'city', 'token_id': 2103, 'type': 'input', 'position': 58}, {'token': ',', 'token_id': 1010, 'type': 'input', 'position': 59}, {'token': 'and', 'token_id': 1998, 'type': 'input', 'position': 60}, {'token': 'then', 'token_id': 2059, 'type': 'input', 'position': 61}, {'token': 'it', 'token_id': 2009, 'type': 'input', 'position': 62}, {'token': 'left', 'token_id': 2187, 'type': 'input', 'position': 63}, {'token': '.', 'token_id': 1012, 'type': 'input', 'position': 64}, {'token': 'the', 'token_id': 1996, 'type': 'input', 'position': 65}, {'token': 'way', 'token_id': 2126, 'type': 'input', 'position': 66}, {'token': 'to', 'token_id': 2000, 'type': 'input', 'position': 67}, {'token': 'end', 'token_id': 2203, 'type': 'input', 'position': 68}, {'token': 'a', 'token_id': 1037, 'type': 'input', 'position': 69}, {'token': 'poem', 'token_id': 5961, 'type': 'input', 'position': 70}, {'token': 'like', 'token_id': 2066, 'type': 'input', 'position': 71}, {'token': 'this', 'token_id': 2023, 'type': 'input', 'position': 72}, {'token': 'is', 'token_id': 2003, 'type': 'input', 'position': 73}, {'token': 'to', 'token_id': 2000, 'type': 'input', 'position': 74}, {'token': 'become', 'token_id': 2468, 'type': 'input', 'position': 75}, {'token': 'suddenly', 'token_id': 3402, 'type': 'input', 'position': 76}, {'token': 'quiet', 'token_id': 4251, 'type': 'input', 'position': 77}, {'token': '.', 'token_id': 1012, 'type': 'input', 'position': 78}, {'token': '[SEP]', 'token_id': 102, 'type': 'input', 'position': 79}], 'factors': [[[0.004870616365224123, 0.0, 0.0, 0.0, 0.07164324820041656, 0.0, 0.0, 0.007287974469363689, 0.0, 0.03608906269073486, 0.0017666865605860949, 0.059347279369831085, 0.1632036715745926, 0.000746491423342377, 0.0015942504396662116, 0.0, 0.0, 0.0, 0.0, 0.02416246198117733, 0.0, 0.0, 0.0, 0.028846021741628647, 0.02202560193836689, 0.02572380006313324, 0.005536071956157684, 0.056417688727378845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.054985467344522476, 0.0, 0.005490402691066265, 0.040943581610918045, 0.07390829175710678, 0.0, 0.2971138656139374, 0.0, 0.15135858952999115, 0.27391287684440613, 0.0, 0.10928841680288315, 0.0, 0.13095979392528534, 0.2666741609573364, 0.014965265989303589, 0.0, 0.11610202491283417, 0.12992824614048004, 0.04513842612504959, 0.2904844284057617, 1.2424508895492181e-05, 0.1092967763543129, 0.20661185681819916, 0.04354960098862648, 0.034822989255189896, 0.0, 0.0, 0.05737120285630226, 0.05629068240523338, 0.05328492447733879, 0.09862210601568222, 0.3113488554954529, 0.14717955887317657, 0.28354090452194214, 0.09815606474876404, 0.3152834475040436, 0.04430893063545227, 0.1554732620716095, 0.21100877225399017, 0.1853393316268921, 0.2590242922306061, 0.11594835668802261, 0.0812423974275589, 0.0313027985394001, 0.0, 0.0], [0.03712913766503334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0029798075556755066, 0.003048685844987631, 0.010811909101903439, 0.0019716881215572357, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.125743624987081e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0012119325110688806, 0.0, 0.010079676285386086, 0.002506712218746543, 0.0004762053722515702, 0.0, 0.0, 0.0, 0.0, 0.006330303847789764, 0.00020840844081249088, 0.24903079867362976, 0.0, 0.0, 0.001231106580235064, 0.0004905466921627522, 0.05100851505994797, 0.010888883844017982, 0.005285820923745632, 0.0, 0.0, 0.0, 0.0051609231159091, 0.0, 0.008161839097738266, 0.011523769237101078, 0.001613251632079482, 0.012221052311360836, 0.011893298476934433, 0.017314622178673744, 0.6664002537727356, 1.7040143013000488], [0.027403073385357857, 0.0, 0.32317236065864563, 0.27194926142692566, 0.06716003268957138, 0.22706292569637299, 0.16266079246997833, 0.10858936607837677, 0.0, 0.1403152197599411, 0.17501501739025116, 0.1579258292913437, 0.056796394288539886, 0.21391354501247406, 0.061137065291404724, 0.0, 0.3244198262691498, 0.011288167908787727, 0.3640992045402527, 0.09386573731899261, 0.022642498835921288, 0.0, 0.24866025149822235, 0.21672065556049347, 0.14884711802005768, 0.1255686730146408, 0.19408920407295227, 0.15666799247264862, 0.2314980924129486, 0.0, 0.2642047703266144, 0.0, 0.20473772287368774, 0.07109726220369339, 0.21736480295658112, 0.10676033049821854, 0.22209791839122772, 0.0686064213514328, 0.3096259534358978, 0.0, 0.2506190538406372, 0.06182930991053581, 0.0, 0.24358490109443665, 0.06982436776161194, 0.2933378517627716, 0.11816397309303284, 0.01481711771339178, 0.22935529053211212, 0.0, 0.0374910868704319, 0.0997268557548523, 0.2488100826740265, 0.0, 0.27230021357536316, 0.11924818903207779, 0.010352319106459618, 0.13362833857536316, 0.18073391914367676, 0.0, 0.0, 0.11308294534683228, 0.0, 0.18548588454723358, 0.0, 0.0, 0.04702252522110939, 0.0, 0.15214022994041443, 0.0, 0.19487956166267395, 0.043261297047138214, 0.00840303860604763, 0.08171477168798447, 0.0, 0.14678746461868286, 0.19029051065444946, 0.20349979400634766, 0.0, 0.0], [0.0328206941485405, 0.7113119959831238, 0.07612116634845734, 0.02728286199271679, 0.20657630264759064, 0.018992384895682335, 0.09074049443006516, 0.059260785579681396, 0.026189446449279785, 0.06652360409498215, 0.0928463488817215, 0.049614742398262024, 0.024005969986319542, 0.01928262785077095, 0.0630241185426712, 0.7488448619842529, 0.039012882858514786, 0.09701558947563171, 0.0, 0.19183406233787537, 0.044705070555210114, 0.6953794956207275, 0.0, 0.043425556272268295, 0.007351011503487825, 0.0, 0.0, 0.009432449005544186, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0002192524989368394, 0.0, 0.012630506418645382, 0.021193088963627815, 0.24463403224945068, 0.0, 0.040004268288612366, 0.0, 0.006033215671777725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17044439911842346, 0.004820323549211025, 0.0, 0.028453515842556953, 0.0, 0.0, 0.08622921258211136, 0.0, 0.008201791904866695, 0.0, 0.0, 0.02131173387169838, 0.6039677262306213, 0.0, 0.0, 0.025939611718058586, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10396979004144669, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07186222821474075, 0.0, 0.0, 0.042264424264431, 0.07426851242780685, 0.0, 0.21541741490364075, 0.2767510414123535, 0.6449134349822998, 0.0193953700363636, 0.029807843267917633, 0.019274944439530373, 0.02507173642516136, 0.0, 0.5237492918968201, 0.0, 0.0, 0.7193576693534851, 0.0, 0.1263854205608368, 0.636256992816925, 0.00629287539049983, 0.0, 0.09984786063432693, 0.091569684445858, 0.0711202323436737, 0.06204836443066597, 0.10006525367498398, 0.0, 0.764751672744751, 0.0, 0.7514910697937012, 0.03291086107492447, 0.3543850779533386, 0.0, 0.43180322647094727, 0.05084460601210594, 0.029667546972632408, 0.0, 0.0, 0.0, 0.10746367275714874, 0.0, 0.0, 0.2221987247467041, 0.0, 0.09049978852272034, 0.0, 0.0, 0.8091963529586792, 0.053500108420848846, 0.04066568985581398, 0.0, 0.0, 0.0, 0.044701602309942245, 0.04009184613823891, 0.030191972851753235, 0.03833050653338432, 0.7461895942687988, 0.8630335330963135, 0.26939651370048523, 0.0, 0.042646270245313644, 0.4728941023349762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20251010358333588, 0.0, 0.06522399932146072, 0.02112133428454399, 0.0, 0.011736067943274975, 0.0, 0.592275857925415, 0.0]]]},\n            {\n            'hltrCFG': {'tokenization_config': {\"token_prefix\": \"\", \"partial_token_prefix\": \"##\"}\n                }\n            })\n         }, function (err) {\n            console.log(err);\n        })",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nmf_1.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: \n",
      "You will act as an explainability module, providing a narrative explanation the relationship between several strings containing words.\n",
      "You will receive several strings of initial text, but with masks.\n",
      "A masked token is marked by 1 underscore.\n",
      "Several underscores mean that several tokens have been replaced.\n",
      "The tokens, which have not been replaced by an underscore, has been shown to be activated by some cluster of neurons of the large language model, in response to the input shown earlier.\n",
      "Your job is to describe any relationship which might be present between the remaining tokens, which are not replaced by underscores.\n",
      "These relationships might be syntactic, grammatic, semantic or some special token of the large language model like \"[CLS]\" and \"[SEP]\".\n",
      "Just saying that \"this factor relates to words\" is insufficient. You must figure out what these tokens have in common.\n",
      "They might attend to particular parts of a sentence like the start, middle or the end.\n",
      "If there is no clear relationship between any of the tokens of a factor you should write: \"No intuitive connection between the tokens of this factor was found.\". \n",
      "Be brief, and only give a few examples.\n",
      "You are to provide the \"output\" similiar to those in the examples. \n",
      "It's crucial that you DO NOT copy the most similiar example, but actually figure out for yourself what the tokens have in common, and report on this.threshold: 0.005\n",
      "n_components: 5\n",
      "Original input string: [CLS]itsitsoutsidemywindownowlikeandoldwomangoingtomarket;itsitsandwatchesme,itsweat##sne##vo##uslythroughwireandfoganddog-barkuntilsuddenlyislamthescreenwithanewspaperlikeslappingataflyandyoucouldhearthescreamoverthisplaincity,andthenitleft.thewaytoendapoemlikethisistobecomesuddenlyquiet.[SEP]\n",
      "Masked string: ____my__like_old_goingto______me___##sne##vo##uslythrough_____-_untilsuddenlyi_the_witha_like_atafly_youcouldhearthe_overthisplaincity__thenitleft.thewaytoendapoemlikethisistobecomesuddenlyquiet__\n",
      "Output: \n",
      "n_components: 5\n",
      "Original input string: [CLS]theroadnottakentworoadsdiver##gedinayellowwood,andsorryicouldnottravelbothandbeonetraveler,longistoodandlookeddownoneasfarasicouldtowhereitbentintheunder##growth;thentooktheother,asjustasfair,andhavingperhapsthebetterclaim,becauseitwasgrassyandwantedwear;thoughasforthatthepassingtherehadwornthemreallyaboutthesame,andboththatmorningequallylayinleavesnostephadtr##od##denblack.oh,ikeptthefirstforanotherday!yetknowinghowwayleadsontoway,idoubtedifishouldevercomeback.ishallbetellingthiswithasighsomewhereagesandageshence:tworoadsdiver##gedinawood,andi—itooktheonelesstraveledby,andthathasmadeallthedifference.[SEP]\n",
      "Masked string: ________________________________________________________,______,______________________,_______________._________!________,__if_____.________________________________,_______._\n",
      "Output: This cluster responds to punctuation like commas, explamation marks and periods.\n",
      "\n",
      "threshold: 0.01\n",
      "n_components: 10\n",
      "Original input string: '[CLS] the road not taken two roads diver ##ged in a yellow wood , and sorry i could not travel both and be one traveler , long i stood and looked down one as far as i could to where it bent in the under ##growth ; then took the other , as just as fair , and having perhaps the better claim , because it was grassy and wanted wear ; though as for that the passing there had worn them really about the same , and both that morning equally lay in leaves no step had tr ##od ##den black . oh , i kept the first for another day ! yet knowing how way leads on to way , i doubted if i should ever come back . i shall be telling this with a sigh somewhere ages and ages hence : two roads diver ##ged in a wood , and i — i took the one less traveled by , and that has made all the difference . [SEP]' .\n",
      "Masked string: \n",
      "'_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ , _ _ _ _ _ _ , _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ , _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . _ _ _ _ _ _ _ _ _ ! _ _ _ _ _ _ _ _ , _ _ _ _ _ _ _ _ . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ , _ _ _ _ _ _ _ . _'. \n",
      "Output: This cluster responds to attends to punctuations \".\", \",\" and \"!\".\n",
      "\n",
      "threshold: 0.01\n",
      "n_components: 10\n",
      "Original input string: '[CLS] the road not taken two roads diver ##ged in a yellow wood , and sorry i could not travel both and be one traveler , long i stood and looked down one as far as i could to where it bent in the under ##growth ; then took the other , as just as fair , and having perhaps the better claim , because it was grassy and wanted wear ; though as for that the passing there had worn them really about the same , and both that morning equally lay in leaves no step had tr ##od ##den black . oh , i kept the first for another day ! yet knowing how way leads on to way , i doubted if i should ever come back . i shall be telling this with a sigh somewhere ages and ages hence : two roads diver ##ged in a wood , and i — i took the one less traveled by , and that has made all the difference . [SEP]' .\n",
      "Masked string: '[CLS] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ , _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; _ _ _ _ _ _ _ _ _ _ _ _ perhaps _ _ _ _ _ _ _ _ _ _ _ ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ oh , _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ i shall _ _ this _ a _ _ _ _ _ _ : _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ all the _ . [SEP]'. \n",
      "Output: This cluster responds to very few tokens, most notably the [SEP] token ending the sequence, and the [CLS] token starting the sequence.\n",
      "Now it's your turn, continue using this input:threshold: 0.005\n",
      "n_components: 5\n",
      "Original input string: [CLS]itsitsoutsidemywindownowlikeandoldwomangoingtomarket;itsitsandwatchesme,itsweat##sne##vo##uslythroughwireandfoganddog-barkuntilsuddenlyislamthescreenwithanewspaperlikeslappingataflyandyoucouldhearthescreamoverthisplaincity,andthenitleft.thewaytoendapoemlikethisistobecomesuddenlyquiet.[SEP]\n",
      "Masked string: ____my__like_old_goingto______me___##sne##vo##uslythrough_____-_untilsuddenlyi_the_witha_like_atafly_youcouldhearthe_overthisplaincity__thenitleft.thewaytoendapoemlikethisistobecomesuddenlyquiet__\n",
      "Output: \n",
      "prompt: \n",
      "You will act as an explainability module, providing a narrative explanation the relationship between several strings containing words.\n",
      "You will receive several strings of initial text, but with masks.\n",
      "A masked token is marked by 1 underscore.\n",
      "Several underscores mean that several tokens have been replaced.\n",
      "The tokens, which have not been replaced by an underscore, has been shown to be activated by some cluster of neurons of the large language model, in response to the input shown earlier.\n",
      "Your job is to describe any relationship which might be present between the remaining tokens, which are not replaced by underscores.\n",
      "These relationships might be syntactic, grammatic, semantic or some special token of the large language model like \"[CLS]\" and \"[SEP]\".\n",
      "Just saying that \"this factor relates to words\" is insufficient. You must figure out what these tokens have in common.\n",
      "They might attend to particular parts of a sentence like the start, middle or the end.\n",
      "If there is no clear relationship between any of the tokens of a factor you should write: \"No intuitive connection between the tokens of this factor was found.\". \n",
      "Be brief, and only give a few examples.\n",
      "You are to provide the \"output\" similiar to those in the examples. \n",
      "It's crucial that you DO NOT copy the most similiar example, but actually figure out for yourself what the tokens have in common, and report on this.threshold: 0.005\n",
      "n_components: 5\n",
      "Original input string: [CLS]itsitsoutsidemywindownowlikeandoldwomangoingtomarket;itsitsandwatchesme,itsweat##sne##vo##uslythroughwireandfoganddog-barkuntilsuddenlyislamthescreenwithanewspaperlikeslappingataflyandyoucouldhearthescreamoverthisplaincity,andthenitleft.thewaytoendapoemlikethisistobecomesuddenlyquiet.[SEP]\n",
      "Masked string: [CLS]________________________##vo________________________you______plain_,____.theway___poem_thisis_becomesuddenlyquiet.[SEP]\n",
      "Output: \n",
      "threshold: 0.01\n",
      "n_components: 10\n",
      "Original input string: '[CLS] the road not taken two roads diver ##ged in a yellow wood , and sorry i could not travel both and be one traveler , long i stood and looked down one as far as i could to where it bent in the under ##growth ; then took the other , as just as fair , and having perhaps the better claim , because it was grassy and wanted wear ; though as for that the passing there had worn them really about the same , and both that morning equally lay in leaves no step had tr ##od ##den black . oh , i kept the first for another day ! yet knowing how way leads on to way , i doubted if i should ever come back . i shall be telling this with a sigh somewhere ages and ages hence : two roads diver ##ged in a wood , and i — i took the one less traveled by , and that has made all the difference . [SEP]' .\n",
      "Masked string: '[CLS] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ , _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; _ _ _ _ _ _ _ _ _ _ _ _ perhaps _ _ _ _ _ _ _ _ _ _ _ ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ oh , _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ i shall _ _ this _ a _ _ _ _ _ _ : _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ all the _ . [SEP]'. \n",
      "Output: This cluster responds to very few tokens, most notably the [SEP] token ending the sequence, and the [CLS] token starting the sequence.\n",
      "\n",
      "n_components: 5\n",
      "Original input string: [CLS]theroadnottakentworoadsdiver##gedinayellowwood,andsorryicouldnottravelbothandbeonetraveler,longistoodandlookeddownoneasfarasicouldtowhereitbentintheunder##growth;thentooktheother,asjustasfair,andhavingperhapsthebetterclaim,becauseitwasgrassyandwantedwear;thoughasforthatthepassingtherehadwornthemreallyaboutthesame,andboththatmorningequallylayinleavesnostephadtr##od##denblack.oh,ikeptthefirstforanotherday!yetknowinghowwayleadsontoway,idoubtedifishouldevercomeback.ishallbetellingthiswithasighsomewhereagesandageshence:tworoadsdiver##gedinawood,andi—itooktheonelesstraveledby,andthathasmadeallthedifference.[SEP]\n",
      "Masked string: ________________________________________________________,______,______________________,_______________._________!________,__if_____.________________________________,_______._\n",
      "Output: This cluster responds to punctuation like commas, explamation marks and periods.\n",
      "\n",
      "threshold: 0.01\n",
      "n_components: 10\n",
      "Original input string: '[CLS] the road not taken two roads diver ##ged in a yellow wood , and sorry i could not travel both and be one traveler , long i stood and looked down one as far as i could to where it bent in the under ##growth ; then took the other , as just as fair , and having perhaps the better claim , because it was grassy and wanted wear ; though as for that the passing there had worn them really about the same , and both that morning equally lay in leaves no step had tr ##od ##den black . oh , i kept the first for another day ! yet knowing how way leads on to way , i doubted if i should ever come back . i shall be telling this with a sigh somewhere ages and ages hence : two roads diver ##ged in a wood , and i — i took the one less traveled by , and that has made all the difference . [SEP]' .\n",
      "Masked string: \n",
      "'_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ , _ _ _ _ _ _ , _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ , _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . _ _ _ _ _ _ _ _ _ ! _ _ _ _ _ _ _ _ , _ _ _ _ _ _ _ _ . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ , _ _ _ _ _ _ _ . _'. \n",
      "Output: This cluster responds to attends to punctuations \".\", \",\" and \"!\".\n",
      "Now it's your turn, continue using this input:threshold: 0.005\n",
      "n_components: 5\n",
      "Original input string: [CLS]itsitsoutsidemywindownowlikeandoldwomangoingtomarket;itsitsandwatchesme,itsweat##sne##vo##uslythroughwireandfoganddog-barkuntilsuddenlyislamthescreenwithanewspaperlikeslappingataflyandyoucouldhearthescreamoverthisplaincity,andthenitleft.thewaytoendapoemlikethisistobecomesuddenlyquiet.[SEP]\n",
      "Masked string: [CLS]________________________##vo________________________you______plain_,____.theway___poem_thisis_becomesuddenlyquiet.[SEP]\n",
      "Output: \n",
      "prompt: \n",
      "You will act as an explainability module, providing a narrative explanation the relationship between several strings containing words.\n",
      "You will receive several strings of initial text, but with masks.\n",
      "A masked token is marked by 1 underscore.\n",
      "Several underscores mean that several tokens have been replaced.\n",
      "The tokens, which have not been replaced by an underscore, has been shown to be activated by some cluster of neurons of the large language model, in response to the input shown earlier.\n",
      "Your job is to describe any relationship which might be present between the remaining tokens, which are not replaced by underscores.\n",
      "These relationships might be syntactic, grammatic, semantic or some special token of the large language model like \"[CLS]\" and \"[SEP]\".\n",
      "Just saying that \"this factor relates to words\" is insufficient. You must figure out what these tokens have in common.\n",
      "They might attend to particular parts of a sentence like the start, middle or the end.\n",
      "If there is no clear relationship between any of the tokens of a factor you should write: \"No intuitive connection between the tokens of this factor was found.\". \n",
      "Be brief, and only give a few examples.\n",
      "You are to provide the \"output\" similiar to those in the examples. \n",
      "It's crucial that you DO NOT copy the most similiar example, but actually figure out for yourself what the tokens have in common, and report on this.threshold: 0.005\n",
      "n_components: 5\n",
      "Original input string: [CLS]itsitsoutsidemywindownowlikeandoldwomangoingtomarket;itsitsandwatchesme,itsweat##sne##vo##uslythroughwireandfoganddog-barkuntilsuddenlyislamthescreenwithanewspaperlikeslappingataflyandyoucouldhearthescreamoverthisplaincity,andthenitleft.thewaytoendapoemlikethisistobecomesuddenlyquiet.[SEP]\n",
      "Masked string: [CLS]_sitsoutsidemywindownowlike_oldwomangoingtomarket;_sitsandwatchesme,_sweat##sne##vo##uslythroughwire_fog_dog-barkuntilsuddenlyislam_screenwith_newspaperlikeslappingatafly_youcouldhear_screamoverthisplaincity__then_left__way_end_poemlikethisis_becomesuddenlyquiet__\n",
      "Output: \n",
      "threshold: 0.01\n",
      "n_components: 10\n",
      "Original input string: '[CLS] the road not taken two roads diver ##ged in a yellow wood , and sorry i could not travel both and be one traveler , long i stood and looked down one as far as i could to where it bent in the under ##growth ; then took the other , as just as fair , and having perhaps the better claim , because it was grassy and wanted wear ; though as for that the passing there had worn them really about the same , and both that morning equally lay in leaves no step had tr ##od ##den black . oh , i kept the first for another day ! yet knowing how way leads on to way , i doubted if i should ever come back . i shall be telling this with a sigh somewhere ages and ages hence : two roads diver ##ged in a wood , and i — i took the one less traveled by , and that has made all the difference . [SEP]' .\n",
      "Masked string: \n",
      "'_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ , _ _ _ _ _ _ , _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ , _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . _ _ _ _ _ _ _ _ _ ! _ _ _ _ _ _ _ _ , _ _ _ _ _ _ _ _ . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ , _ _ _ _ _ _ _ . _'. \n",
      "Output: This cluster responds to attends to punctuations \".\", \",\" and \"!\".\n",
      "\n",
      "threshold: 0.01\n",
      "n_components: 10\n",
      "Original input string: '[CLS] the road not taken two roads diver ##ged in a yellow wood , and sorry i could not travel both and be one traveler , long i stood and looked down one as far as i could to where it bent in the under ##growth ; then took the other , as just as fair , and having perhaps the better claim , because it was grassy and wanted wear ; though as for that the passing there had worn them really about the same , and both that morning equally lay in leaves no step had tr ##od ##den black . oh , i kept the first for another day ! yet knowing how way leads on to way , i doubted if i should ever come back . i shall be telling this with a sigh somewhere ages and ages hence : two roads diver ##ged in a wood , and i — i took the one less traveled by , and that has made all the difference . [SEP]' .\n",
      "Masked string: '[CLS] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ , _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; _ _ _ _ _ _ _ _ _ _ _ _ perhaps _ _ _ _ _ _ _ _ _ _ _ ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ oh , _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ i shall _ _ this _ a _ _ _ _ _ _ : _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ all the _ . [SEP]'. \n",
      "Output: This cluster responds to very few tokens, most notably the [SEP] token ending the sequence, and the [CLS] token starting the sequence.\n",
      "\n",
      "n_components: 5\n",
      "Original input string: [CLS]theroadnottakentworoadsdiver##gedinayellowwood,andsorryicouldnottravelbothandbeonetraveler,longistoodandlookeddownoneasfarasicouldtowhereitbentintheunder##growth;thentooktheother,asjustasfair,andhavingperhapsthebetterclaim,becauseitwasgrassyandwantedwear;thoughasforthatthepassingtherehadwornthemreallyaboutthesame,andboththatmorningequallylayinleavesnostephadtr##od##denblack.oh,ikeptthefirstforanotherday!yetknowinghowwayleadsontoway,idoubtedifishouldevercomeback.ishallbetellingthiswithasighsomewhereagesandageshence:tworoadsdiver##gedinawood,andi—itooktheonelesstraveledby,andthathasmadeallthedifference.[SEP]\n",
      "Masked string: ________________________________________________________,______,______________________,_______________._________!________,__if_____.________________________________,_______._\n",
      "Output: This cluster responds to punctuation like commas, explamation marks and periods.\n",
      "Now it's your turn, continue using this input:threshold: 0.005\n",
      "n_components: 5\n",
      "Original input string: [CLS]itsitsoutsidemywindownowlikeandoldwomangoingtomarket;itsitsandwatchesme,itsweat##sne##vo##uslythroughwireandfoganddog-barkuntilsuddenlyislamthescreenwithanewspaperlikeslappingataflyandyoucouldhearthescreamoverthisplaincity,andthenitleft.thewaytoendapoemlikethisistobecomesuddenlyquiet.[SEP]\n",
      "Masked string: [CLS]_sitsoutsidemywindownowlike_oldwomangoingtomarket;_sitsandwatchesme,_sweat##sne##vo##uslythroughwire_fog_dog-barkuntilsuddenlyislam_screenwith_newspaperlikeslappingatafly_youcouldhear_screamoverthisplaincity__then_left__way_end_poemlikethisis_becomesuddenlyquiet__\n",
      "Output: \n",
      "prompt: \n",
      "You will act as an explainability module, providing a narrative explanation the relationship between several strings containing words.\n",
      "You will receive several strings of initial text, but with masks.\n",
      "A masked token is marked by 1 underscore.\n",
      "Several underscores mean that several tokens have been replaced.\n",
      "The tokens, which have not been replaced by an underscore, has been shown to be activated by some cluster of neurons of the large language model, in response to the input shown earlier.\n",
      "Your job is to describe any relationship which might be present between the remaining tokens, which are not replaced by underscores.\n",
      "These relationships might be syntactic, grammatic, semantic or some special token of the large language model like \"[CLS]\" and \"[SEP]\".\n",
      "Just saying that \"this factor relates to words\" is insufficient. You must figure out what these tokens have in common.\n",
      "They might attend to particular parts of a sentence like the start, middle or the end.\n",
      "If there is no clear relationship between any of the tokens of a factor you should write: \"No intuitive connection between the tokens of this factor was found.\". \n",
      "Be brief, and only give a few examples.\n",
      "You are to provide the \"output\" similiar to those in the examples. \n",
      "It's crucial that you DO NOT copy the most similiar example, but actually figure out for yourself what the tokens have in common, and report on this.threshold: 0.005\n",
      "n_components: 5\n",
      "Original input string: [CLS]itsitsoutsidemywindownowlikeandoldwomangoingtomarket;itsitsandwatchesme,itsweat##sne##vo##uslythroughwireandfoganddog-barkuntilsuddenlyislamthescreenwithanewspaperlikeslappingataflyandyoucouldhearthescreamoverthisplaincity,andthenitleft.thewaytoendapoemlikethisistobecomesuddenlyquiet.[SEP]\n",
      "Masked string: [CLS]itsitsoutsidemywindownowlikeandoldwomangoingtomarket;itsitsand_me,it_##sne__through_______untilsuddenlyi_the_with________you__the__this_city__thenit__the______this_______\n",
      "Output: \n",
      "threshold: 0.01\n",
      "n_components: 10\n",
      "Original input string: '[CLS] the road not taken two roads diver ##ged in a yellow wood , and sorry i could not travel both and be one traveler , long i stood and looked down one as far as i could to where it bent in the under ##growth ; then took the other , as just as fair , and having perhaps the better claim , because it was grassy and wanted wear ; though as for that the passing there had worn them really about the same , and both that morning equally lay in leaves no step had tr ##od ##den black . oh , i kept the first for another day ! yet knowing how way leads on to way , i doubted if i should ever come back . i shall be telling this with a sigh somewhere ages and ages hence : two roads diver ##ged in a wood , and i — i took the one less traveled by , and that has made all the difference . [SEP]' .\n",
      "Masked string: \n",
      "'_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ , _ _ _ _ _ _ , _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ , _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . _ _ _ _ _ _ _ _ _ ! _ _ _ _ _ _ _ _ , _ _ _ _ _ _ _ _ . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ , _ _ _ _ _ _ _ . _'. \n",
      "Output: This cluster responds to attends to punctuations \".\", \",\" and \"!\".\n",
      "\n",
      "n_components: 5\n",
      "Original input string: [CLS]theroadnottakentworoadsdiver##gedinayellowwood,andsorryicouldnottravelbothandbeonetraveler,longistoodandlookeddownoneasfarasicouldtowhereitbentintheunder##growth;thentooktheother,asjustasfair,andhavingperhapsthebetterclaim,becauseitwasgrassyandwantedwear;thoughasforthatthepassingtherehadwornthemreallyaboutthesame,andboththatmorningequallylayinleavesnostephadtr##od##denblack.oh,ikeptthefirstforanotherday!yetknowinghowwayleadsontoway,idoubtedifishouldevercomeback.ishallbetellingthiswithasighsomewhereagesandageshence:tworoadsdiver##gedinawood,andi—itooktheonelesstraveledby,andthathasmadeallthedifference.[SEP]\n",
      "Masked string: ________________________________________________________,______,______________________,_______________._________!________,__if_____.________________________________,_______._\n",
      "Output: This cluster responds to punctuation like commas, explamation marks and periods.\n",
      "\n",
      "threshold: 0.01\n",
      "n_components: 10\n",
      "Original input string: '[CLS] the road not taken two roads diver ##ged in a yellow wood , and sorry i could not travel both and be one traveler , long i stood and looked down one as far as i could to where it bent in the under ##growth ; then took the other , as just as fair , and having perhaps the better claim , because it was grassy and wanted wear ; though as for that the passing there had worn them really about the same , and both that morning equally lay in leaves no step had tr ##od ##den black . oh , i kept the first for another day ! yet knowing how way leads on to way , i doubted if i should ever come back . i shall be telling this with a sigh somewhere ages and ages hence : two roads diver ##ged in a wood , and i — i took the one less traveled by , and that has made all the difference . [SEP]' .\n",
      "Masked string: '[CLS] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ , _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; _ _ _ _ _ _ _ _ _ _ _ _ perhaps _ _ _ _ _ _ _ _ _ _ _ ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ oh , _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ i shall _ _ this _ a _ _ _ _ _ _ : _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ all the _ . [SEP]'. \n",
      "Output: This cluster responds to very few tokens, most notably the [SEP] token ending the sequence, and the [CLS] token starting the sequence.\n",
      "Now it's your turn, continue using this input:threshold: 0.005\n",
      "n_components: 5\n",
      "Original input string: [CLS]itsitsoutsidemywindownowlikeandoldwomangoingtomarket;itsitsandwatchesme,itsweat##sne##vo##uslythroughwireandfoganddog-barkuntilsuddenlyislamthescreenwithanewspaperlikeslappingataflyandyoucouldhearthescreamoverthisplaincity,andthenitleft.thewaytoendapoemlikethisistobecomesuddenlyquiet.[SEP]\n",
      "Masked string: [CLS]itsitsoutsidemywindownowlikeandoldwomangoingtomarket;itsitsand_me,it_##sne__through_______untilsuddenlyi_the_with________you__the__this_city__thenit__the______this_______\n",
      "Output: \n",
      "prompt: \n",
      "You will act as an explainability module, providing a narrative explanation the relationship between several strings containing words.\n",
      "You will receive several strings of initial text, but with masks.\n",
      "A masked token is marked by 1 underscore.\n",
      "Several underscores mean that several tokens have been replaced.\n",
      "The tokens, which have not been replaced by an underscore, has been shown to be activated by some cluster of neurons of the large language model, in response to the input shown earlier.\n",
      "Your job is to describe any relationship which might be present between the remaining tokens, which are not replaced by underscores.\n",
      "These relationships might be syntactic, grammatic, semantic or some special token of the large language model like \"[CLS]\" and \"[SEP]\".\n",
      "Just saying that \"this factor relates to words\" is insufficient. You must figure out what these tokens have in common.\n",
      "They might attend to particular parts of a sentence like the start, middle or the end.\n",
      "If there is no clear relationship between any of the tokens of a factor you should write: \"No intuitive connection between the tokens of this factor was found.\". \n",
      "Be brief, and only give a few examples.\n",
      "You are to provide the \"output\" similiar to those in the examples. \n",
      "It's crucial that you DO NOT copy the most similiar example, but actually figure out for yourself what the tokens have in common, and report on this.threshold: 0.005\n",
      "n_components: 5\n",
      "Original input string: [CLS]itsitsoutsidemywindownowlikeandoldwomangoingtomarket;itsitsandwatchesme,itsweat##sne##vo##uslythroughwireandfoganddog-barkuntilsuddenlyislamthescreenwithanewspaperlikeslappingataflyandyoucouldhearthescreamoverthisplaincity,andthenitleft.thewaytoendapoemlikethisistobecomesuddenlyquiet.[SEP]\n",
      "Masked string: [CLS]__outsidemy_nowlikeandoldwomangoingto_;__and_me,it_##sne##vo##uslythrough_and_anddog-_untilsuddenlyi___with__like_at__andyoucould___overthisplaincity,andthen_left.______like_isto_suddenly_._\n",
      "Output: \n",
      "threshold: 0.01\n",
      "n_components: 10\n",
      "Original input string: '[CLS] the road not taken two roads diver ##ged in a yellow wood , and sorry i could not travel both and be one traveler , long i stood and looked down one as far as i could to where it bent in the under ##growth ; then took the other , as just as fair , and having perhaps the better claim , because it was grassy and wanted wear ; though as for that the passing there had worn them really about the same , and both that morning equally lay in leaves no step had tr ##od ##den black . oh , i kept the first for another day ! yet knowing how way leads on to way , i doubted if i should ever come back . i shall be telling this with a sigh somewhere ages and ages hence : two roads diver ##ged in a wood , and i — i took the one less traveled by , and that has made all the difference . [SEP]' .\n",
      "Masked string: \n",
      "'_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ , _ _ _ _ _ _ , _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ , _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . _ _ _ _ _ _ _ _ _ ! _ _ _ _ _ _ _ _ , _ _ _ _ _ _ _ _ . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ , _ _ _ _ _ _ _ . _'. \n",
      "Output: This cluster responds to attends to punctuations \".\", \",\" and \"!\".\n",
      "\n",
      "threshold: 0.01\n",
      "n_components: 10\n",
      "Original input string: '[CLS] the road not taken two roads diver ##ged in a yellow wood , and sorry i could not travel both and be one traveler , long i stood and looked down one as far as i could to where it bent in the under ##growth ; then took the other , as just as fair , and having perhaps the better claim , because it was grassy and wanted wear ; though as for that the passing there had worn them really about the same , and both that morning equally lay in leaves no step had tr ##od ##den black . oh , i kept the first for another day ! yet knowing how way leads on to way , i doubted if i should ever come back . i shall be telling this with a sigh somewhere ages and ages hence : two roads diver ##ged in a wood , and i — i took the one less traveled by , and that has made all the difference . [SEP]' .\n",
      "Masked string: '[CLS] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ , _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; _ _ _ _ _ _ _ _ _ _ _ _ perhaps _ _ _ _ _ _ _ _ _ _ _ ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ oh , _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ i shall _ _ this _ a _ _ _ _ _ _ : _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ all the _ . [SEP]'. \n",
      "Output: This cluster responds to very few tokens, most notably the [SEP] token ending the sequence, and the [CLS] token starting the sequence.\n",
      "\n",
      "n_components: 5\n",
      "Original input string: [CLS]theroadnottakentworoadsdiver##gedinayellowwood,andsorryicouldnottravelbothandbeonetraveler,longistoodandlookeddownoneasfarasicouldtowhereitbentintheunder##growth;thentooktheother,asjustasfair,andhavingperhapsthebetterclaim,becauseitwasgrassyandwantedwear;thoughasforthatthepassingtherehadwornthemreallyaboutthesame,andboththatmorningequallylayinleavesnostephadtr##od##denblack.oh,ikeptthefirstforanotherday!yetknowinghowwayleadsontoway,idoubtedifishouldevercomeback.ishallbetellingthiswithasighsomewhereagesandageshence:tworoadsdiver##gedinawood,andi—itooktheonelesstraveledby,andthathasmadeallthedifference.[SEP]\n",
      "Masked string: ________________________________________________________,______,______________________,_______________._________!________,__if_____.________________________________,_______._\n",
      "Output: This cluster responds to punctuation like commas, explamation marks and periods.\n",
      "Now it's your turn, continue using this input:threshold: 0.005\n",
      "n_components: 5\n",
      "Original input string: [CLS]itsitsoutsidemywindownowlikeandoldwomangoingtomarket;itsitsandwatchesme,itsweat##sne##vo##uslythroughwireandfoganddog-barkuntilsuddenlyislamthescreenwithanewspaperlikeslappingataflyandyoucouldhearthescreamoverthisplaincity,andthenitleft.thewaytoendapoemlikethisistobecomesuddenlyquiet.[SEP]\n",
      "Masked string: [CLS]__outsidemy_nowlikeandoldwomangoingto_;__and_me,it_##sne##vo##uslythrough_and_anddog-_untilsuddenlyi___with__like_at__andyoucould___overthisplaincity,andthen_left.______like_isto_suddenly_._\n",
      "Output: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\nThis cluster responds to words related to sound and movement, such as \"snevously\", \"bark\", \"slam\", \"lapping\", \"scream\" and \"quiet\". These words suggest a sense of urgency and suddenness, which is reinforced by words like \"suddenly\" and \"quiet\".',\n",
       " '\\nThis cluster responds to the punctuation \",\" and the verbs \"itsits\" and \"you\". It also attends to the words \"vo\", \"plain\" and \"way\" which may suggest the idea of a journey or path.',\n",
       " '\\nThis cluster responds to verbs, like \"sits\", \"watches\", \"lams\" and \"left\", and to adverbs, like \"suddenly\" and \"quietly\".',\n",
       " '\\nThis cluster responds to words which typically appear at the end of a sentence or phrase such as \"left\", \"quiet\" and \"city\".',\n",
       " '\\nThis cluster responds to words related to suddenness and quietness, like \"suddenly\", \"quiet\", and \"left\".']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanations = nmf_1.explain()\n",
    "explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['\\nThis cluster responds to words related to actions, such as \"going\", \"slam\", \"lapping\", \"scream\" and \"left\". These words describe the actions of the old woman outside the window, as well as the narrator\\'s reaction to these actions.',\n",
    " '\\nThis cluster attends to words related to the end of a poem, like \"suddenly\" and \"quiet\". It also attends to punctuations like \",\" and \".\".',\n",
    " '\\nThis cluster responds to punctuations like \",\" and \";\". It also attends to verbs like \"sits\", \"likes\" and \"hear\".',\n",
    " '\\nThis cluster responds to common words such as \"it\", \"me\", \"with\", \"you\", \"the\", \"this\", and \"then\". It also responds to the punctuation mark \",\".',\n",
    " '\\nThis cluster responds to verbs like \"going\", \"watching\", \"slamming\" and \"left\".']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['\\nThis cluster responds to words related to sound and movement, such as \"snevously\", \"bark\", \"slam\", \"lapping\", \"scream\" and \"quiet\". These words suggest a sense of urgency and suddenness, which is reinforced by words like \"suddenly\" and \"quiet\".',\n",
    " '\\nThis cluster responds to the punctuation \",\" and the verbs \"itsits\" and \"you\". It also attends to the words \"vo\", \"plain\" and \"way\" which may suggest the idea of a journey or path.',\n",
    " '\\nThis cluster responds to verbs, like \"sits\", \"watches\", \"lams\" and \"left\", and to adverbs, like \"suddenly\" and \"quietly\".',\n",
    " '\\nThis cluster responds to words which typically appear at the end of a sentence or phrase such as \"left\", \"quiet\" and \"city\".',\n",
    " '\\nThis cluster responds to words related to suddenness and quietness, like \"suddenly\", \"quiet\", and \"left\".']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
